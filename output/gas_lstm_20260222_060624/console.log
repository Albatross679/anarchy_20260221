============================================================
LSTM Gas Consumption Prediction — gas_lstm
Output: output/gas_lstm_20260222_060624
Utility: GAS
Seq length: 48 (12.0h)
Stride: 4
LSTM: hidden=256, layers=3, dropout=0.3
LR: 0.001, weight_decay=0.0001
Batch size: 512
Patience: 15
Seed: 42
============================================================

--- Loading Data ---
  Loaded: 751,048 rows, 48 cols, 146 buildings
  Dropping 12 sparse cross-utility columns
  Active: 115 buildings (590,968 rows)
  Always-off: 31 buildings (160,080 rows)
  Train: 250,940 | Test: 340,028
  Temporal features (28): ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'direct_radiation', 'wind_speed_10m', 'cloud_cover', 'apparent_temperature', 'precipitation']...
  Static features (3): ['grossarea', 'floorsaboveground', 'building_age']

--- Creating Sequence Datasets ---
  Train windows: 61,470
  Test windows:  83,742

--- Model ---
  Parameters: 1,393,185
  Device: cuda
EnergyLSTMHybrid(
  (lstm): LSTM(28, 256, num_layers=3, batch_first=True, dropout=0.3)
  (static_mlp): Sequential(
    (0): Linear(in_features=3, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=32, bias=True)
    (4): ReLU()
  )
  (head): Sequential(
    (0): Linear(in_features=288, out_features=128, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=64, bias=True)
    (4): GELU(approximate='none')
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=64, out_features=1, bias=True)
  )
)

--- Training ---
  Epoch   1/100  train_loss=0.344286  val_loss=0.526807  R²=0.6334  lr=1.00e-03  patience=0/15
  Epoch   5/100  train_loss=0.195509  val_loss=0.492641  R²=0.6570  lr=9.96e-04  patience=1/15
  Epoch  10/100  train_loss=0.094670  val_loss=0.243191  R²=0.8306  lr=9.80e-04  patience=0/15
  Epoch  15/100  train_loss=0.054267  val_loss=0.144680  R²=0.8995  lr=9.52e-04  patience=1/15
  Epoch  20/100  train_loss=0.031641  val_loss=0.072585  R²=0.9495  lr=9.14e-04  patience=0/15
  Epoch  25/100  train_loss=0.026684  val_loss=0.064350  R²=0.9552  lr=8.64e-04  patience=2/15
  Epoch  30/100  train_loss=0.023555  val_loss=0.060242  R²=0.9580  lr=8.06e-04  patience=7/15
  Epoch  35/100  train_loss=0.021426  val_loss=0.050074  R²=0.9651  lr=7.41e-04  patience=0/15
  Epoch  40/100  train_loss=0.020732  val_loss=0.058000  R²=0.9596  lr=6.69e-04  patience=5/15
  Epoch  45/100  train_loss=0.019264  val_loss=0.064713  R²=0.9550  lr=5.94e-04  patience=10/15
  Epoch  50/100  train_loss=0.017819  val_loss=0.055987  R²=0.9610  lr=5.16e-04  patience=3/15
  Epoch  55/100  train_loss=0.016754  val_loss=0.052749  R²=0.9633  lr=4.37e-04  patience=8/15
  Epoch  60/100  train_loss=0.016141  val_loss=0.043779  R²=0.9695  lr=3.61e-04  patience=2/15
  Epoch  65/100  train_loss=0.015129  val_loss=0.048819  R²=0.9660  lr=2.87e-04  patience=7/15
  Epoch  70/100  train_loss=0.014276  val_loss=0.050019  R²=0.9651  lr=2.19e-04  patience=12/15
  Early stopping at epoch 73

--- Evaluation ---
  RMSE:  0.000030
  MAE:   0.000009
  R²:    0.9723
  MAPE:  768.89%

  Model saved: output/gas_lstm_20260222_060624/checkpoints/model_best.pt

--- Generating Predictions ---
  Predictions saved: output/gas_lstm_20260222_060624/predictions.parquet

============================================================
Done in 1995.0s
Output directory: output/gas_lstm_20260222_060624
TensorBoard:  tensorboard --logdir output/gas_lstm_20260222_060624/tensorboard
============================================================
