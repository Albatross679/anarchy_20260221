============================================================
Transformer Gas Consumption Prediction — gas_transformer
Output: output/gas_transformer_20260222_064633
Utility: GAS
Seq length: 48 (12.0h)
Stride: 4
Transformer: d_model=128, heads=4, layers=4, d_ff=256
LR: 0.001, weight_decay=0.0001
Batch size: 512
Patience: 15
Seed: 42
============================================================

--- Loading Data ---
  Loaded: 751,048 rows, 48 cols, 146 buildings
  Dropping 12 sparse cross-utility columns
  Active: 115 buildings (590,968 rows)
  Always-off: 31 buildings (160,080 rows)
  Train: 250,940 | Test: 340,028
  Features (30): ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'direct_radiation', 'wind_speed_10m', 'cloud_cover', 'apparent_temperature', 'precipitation']...

--- Creating Sequence Datasets ---
  Train windows: 61,470
  Test windows:  83,742

--- Model ---
  Parameters: 558,721
  Device: cuda
EnergyTransformer(
  (input_proj): Linear(in_features=30, out_features=128, bias=True)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-3): 4 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (fc): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=64, bias=True)
    (4): GELU(approximate='none')
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=64, out_features=1, bias=True)
  )
)

--- Training ---
  Epoch   1/100  train_loss=0.350485  val_loss=0.556105  R²=0.6125  lr=1.00e-03  patience=0/15
  Epoch   5/100  train_loss=0.248210  val_loss=0.443242  R²=0.6911  lr=9.96e-04  patience=0/15
  Epoch  10/100  train_loss=0.239624  val_loss=0.384423  R²=0.7322  lr=9.80e-04  patience=2/15
  Epoch  15/100  train_loss=0.176640  val_loss=0.324747  R²=0.7737  lr=9.52e-04  patience=0/15
  Epoch  20/100  train_loss=0.146841  val_loss=0.286780  R²=0.8002  lr=9.14e-04  patience=1/15
  Epoch  25/100  train_loss=0.131485  val_loss=0.233630  R²=0.8373  lr=8.64e-04  patience=0/15
  Epoch  30/100  train_loss=0.125223  val_loss=0.280264  R²=0.8049  lr=8.06e-04  patience=4/15
  Epoch  35/100  train_loss=0.110194  val_loss=0.208528  R²=0.8548  lr=7.41e-04  patience=0/15
